{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "5dbdb6af-f077-4eea-82d0-62778e72fc0e",
      "metadata": {
        "id": "5dbdb6af-f077-4eea-82d0-62778e72fc0e"
      },
      "source": [
        "# Parsbert "
      ]
    },
    {
      "cell_type": "markdown",
      "id": "3ed27015-0fec-4651-a09b-2b45e70d7e4b",
      "metadata": {
        "id": "3ed27015-0fec-4651-a09b-2b45e70d7e4b"
      },
      "source": [
        "### Libraries"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "id": "17f6beac-9687-4706-996e-fba077763fbe",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "17f6beac-9687-4706-996e-fba077763fbe",
        "outputId": "e148ad29-9a78-45d8-dede-3d072cab030e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 1
        }
      ],
      "source": [
        "# !pip install transformers\n",
        "# ! pip install nltk\n",
        "import nltk\n",
        "nltk.download('punkt')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "id": "04187156-e7e8-49e8-9b3a-913de490c9ce",
      "metadata": {
        "id": "04187156-e7e8-49e8-9b3a-913de490c9ce"
      },
      "outputs": [],
      "source": [
        "# ! wget https://raw.githubusercontent.com/language-ml/2-LM-embedding-projects/main/problem3/evaluation_IR.yml -P ./data\n",
        "# ! wget https://github.com/language-ml/2-LM-embedding-projects/raw/main/problem3/doc_collection.zip -P ./data\n",
        "# ! unzip ./data/doc_collection.zip -d ./data"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "bb94246e-3d21-4d5c-abbb-d43e6bc7ea2f",
      "metadata": {
        "id": "bb94246e-3d21-4d5c-abbb-d43e6bc7ea2f"
      },
      "source": [
        "### Read data"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "5b200c3d-f60f-4d7f-a875-bf7809f81d18",
      "metadata": {
        "id": "5b200c3d-f60f-4d7f-a875-bf7809f81d18"
      },
      "source": [
        "set path of data in `PATH` variable"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "id": "f1ac1a7a-3783-4d3e-af89-a90b53bdbef2",
      "metadata": {
        "id": "f1ac1a7a-3783-4d3e-af89-a90b53bdbef2"
      },
      "outputs": [],
      "source": [
        "PATH = './data/IR_dataset/'\n",
        "PATH = PATH.rstrip('/')"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "238f5e25-634a-42a7-9534-60dc0ff1ac2b",
      "metadata": {
        "id": "238f5e25-634a-42a7-9534-60dc0ff1ac2b"
      },
      "source": [
        "store txt files into a list named `doc`"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "id": "0a7e16b1-c111-45b4-abd7-5ecd05e55e6d",
      "metadata": {
        "id": "0a7e16b1-c111-45b4-abd7-5ecd05e55e6d"
      },
      "outputs": [],
      "source": [
        "from nltk.tokenize import sent_tokenize\n",
        "\n",
        "docs = []\n",
        "for index in range(0, 3258):\n",
        "    with open(f\"{PATH}/{index}.txt\", 'r', encoding='utf8') as file_reader:\n",
        "      doc = file_reader.read()\n",
        "      doc = sent_tokenize(doc)\n",
        "      docs.append(doc)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "docs = [sent for doc in docs for sent in doc]"
      ],
      "metadata": {
        "id": "6aw1rzV05xSf"
      },
      "id": "6aw1rzV05xSf",
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "len(docs)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SOM3HZSv6h4k",
        "outputId": "c47885ff-ef5f-48e7-cb35-79c86796ba8a"
      },
      "id": "SOM3HZSv6h4k",
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "67319"
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "markdown",
      "id": "f3ae2c52-e92e-4b05-ad30-939ec520163d",
      "metadata": {
        "id": "f3ae2c52-e92e-4b05-ad30-939ec520163d"
      },
      "source": [
        "### Check GPU Availability"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "id": "a6d8d1f8-f4f9-4609-b9c3-8013c91eed56",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "a6d8d1f8-f4f9-4609-b9c3-8013c91eed56",
        "outputId": "a365c49d-2220-4838-b2f8-419d64960327"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 7
        }
      ],
      "source": [
        "import torch\n",
        "torch.cuda.is_available()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "b1bd952e-f5a0-476c-9a57-7629ad11dea3",
      "metadata": {
        "tags": [],
        "id": "b1bd952e-f5a0-476c-9a57-7629ad11dea3"
      },
      "source": [
        "### Implementation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "id": "6bcfd260-32bf-44cd-b084-3c3f37249add",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6bcfd260-32bf-44cd-b084-3c3f37249add",
        "outputId": "dd955015-50d1-4df3-c7fb-17fd330f858f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of the model checkpoint at HooshvareLab/bert-base-parsbert-uncased were not used when initializing BertForQuestionAnswering: ['cls.predictions.decoder.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.bias', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.bias']\n",
            "- This IS expected if you are initializing BertForQuestionAnswering from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing BertForQuestionAnswering from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "Some weights of BertForQuestionAnswering were not initialized from the model checkpoint at HooshvareLab/bert-base-parsbert-uncased and are newly initialized: ['qa_outputs.bias', 'qa_outputs.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        }
      ],
      "source": [
        "from transformers import AutoConfig, AutoTokenizer, BertForQuestionAnswering\n",
        "\n",
        "Model = \"HooshvareLab/bert-base-parsbert-uncased\"\n",
        "config = AutoConfig.from_pretrained(Model)\n",
        "tokenizer = AutoTokenizer.from_pretrained(Model)\n",
        "model = BertForQuestionAnswering.from_pretrained(Model)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Sentence embeddings:\")\n",
        "print(model.parameters())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "M_GnZiicAfq3",
        "outputId": "bc438a59-98e4-4eee-e1c7-f16b5a2816b5"
      },
      "id": "M_GnZiicAfq3",
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Sentence embeddings:\n",
            "<generator object Module.parameters at 0x7fbb601ebc50>\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Mean Pooling - Take attention mask into account for correct averaging\n",
        "def mean_pooling(model_output, attention_mask):\n",
        "    token_embeddings = model_output[0] #First element of model_output contains all token embeddings\n",
        "    input_mask_expanded = attention_mask.unsqueeze(-1).expand(token_embeddings.size()).float()\n",
        "    return torch.sum(token_embeddings * input_mask_expanded, 1) / torch.clamp(input_mask_expanded.sum(1), min=1e-9)"
      ],
      "metadata": {
        "id": "HhjpEpcE22Jl"
      },
      "id": "HhjpEpcE22Jl",
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "id": "8e658d21-44fa-4db4-97dc-096c06cc5245",
      "metadata": {
        "id": "8e658d21-44fa-4db4-97dc-096c06cc5245"
      },
      "outputs": [],
      "source": [
        "out_docs = []\n",
        "for sent in docs:\n",
        "  if len(sent.split())>511:\n",
        "    continue\n",
        "  out_docs.append(sent)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "tokens = tokenizer(sent, return_tensors='pt', padding=True, truncation=True)\n",
        "\n",
        "# contextualized embedding\n",
        "with torch.no_grad():\n",
        "    output_model = model(**tokens)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0AcD86wN-dMr",
        "outputId": "f546951a-6a22-455f-cd90-fc52ae62564f"
      },
      "id": "0AcD86wN-dMr",
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "output_model"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_qxatyYeewi2",
        "outputId": "b51df517-9fbd-46bf-8f13-3127b53d81d0"
      },
      "id": "_qxatyYeewi2",
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "QuestionAnsweringModelOutput([('start_logits',\n",
              "                               tensor([[-0.8131,  0.3454, -0.0252, -0.0262,  0.3414,  0.1872,  1.0214,  0.7793,\n",
              "                                         1.2127,  0.5870,  0.0679, -0.2488,  0.1204,  0.4581,  0.2372,  0.1739,\n",
              "                                        -0.0944, -0.1171]])),\n",
              "                              ('end_logits',\n",
              "                               tensor([[-0.5771, -1.1046, -0.1109,  0.0591,  0.0641,  0.2661, -0.3577,  0.7499,\n",
              "                                        -0.4978, -1.4171, -0.9818, -0.3441, -0.8393,  0.1422,  0.4629,  0.0050,\n",
              "                                         0.0696,  0.0614]]))])"
            ]
          },
          "metadata": {},
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Perform pooling. In this case, mean pooling.\n",
        "sentence_embeddings = mean_pooling(output_model, tokens['attention_mask'])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 305
        },
        "id": "pfUi2DXx_D6q",
        "outputId": "d7c32974-cefb-4889-87d2-1b92d0465356"
      },
      "id": "pfUi2DXx_D6q",
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "error",
          "ename": "RuntimeError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-13-20ef00e19ae3>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# Perform pooling. In this case, mean pooling.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0msentence_embeddings\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmean_pooling\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput_model\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtokens\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'attention_mask'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-10-aa4d2082e550>\u001b[0m in \u001b[0;36mmean_pooling\u001b[0;34m(model_output, attention_mask)\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mmean_pooling\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel_output\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mattention_mask\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m     \u001b[0mtoken_embeddings\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel_output\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;31m#First element of model_output contains all token embeddings\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m     \u001b[0minput_mask_expanded\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mattention_mask\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munsqueeze\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexpand\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtoken_embeddings\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfloat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtoken_embeddings\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0minput_mask_expanded\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclamp\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_mask_expanded\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmin\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1e-9\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mRuntimeError\u001b[0m: expand(torch.LongTensor{[1, 18, 1]}, size=[1, 18]): the number of sizes provided (2) must be greater or equal to the number of dimensions in the tensor (3)"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import pipeline\n",
        "\n",
        "fill = pipeline('fill-mask', model=Model, tokenizer=Model)\n",
        "results = fill('تهران پایتخت [MASK] است.')\n",
        "print(results[0]['token_str'])"
      ],
      "metadata": {
        "id": "ayhN2QXrvU_g"
      },
      "id": "ayhN2QXrvU_g",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Sentence embeddings:\")\n",
        "print(sentence_embeddings.shape)"
      ],
      "metadata": {
        "id": "BVgNimlRw7nB"
      },
      "id": "BVgNimlRw7nB",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "docs[:10]"
      ],
      "metadata": {
        "id": "T4lmco1BUNJs"
      },
      "id": "T4lmco1BUNJs",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "bag = list(set([item for sentence in docs for item in sentence.split(' ') if item != '']))\n",
        "bag_size = len(bag)\n",
        "bag_size"
      ],
      "metadata": {
        "id": "ZVNa9LMkUngG"
      },
      "id": "ZVNa9LMkUngG",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import random\n",
        "\n",
        "sentence_a = []\n",
        "sentence_b = []\n",
        "label = []\n",
        "\n",
        "for sentence in docs:\n",
        "    num_sentences = len(sentence)\n",
        "    if num_sentences > 1:\n",
        "        start = random.randint(0, num_sentences-2)\n",
        "        # 50/50 whether is IsNextSentence or NotNextSentence\n",
        "        if random.random() >= 0.5:\n",
        "            # this is IsNextSentence\n",
        "            sentence_a.append(sentence[start])\n",
        "            sentence_b.append(sentence[start+1])\n",
        "            label.append(0)\n",
        "        else:\n",
        "            index = random.randint(0, bag_size-1)\n",
        "            # this is NotNextSentence\n",
        "            sentence_a.append(sentence[start])\n",
        "            sentence_b.append(bag[index])\n",
        "            label.append(1)"
      ],
      "metadata": {
        "id": "ni-aJ9CPGk18"
      },
      "id": "ni-aJ9CPGk18",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "inputs = tokenizer(sentence_a, sentence_b, return_tensors='pt', max_length=512, truncation=True, padding='max_length')"
      ],
      "metadata": {
        "id": "7sTgXGCEU_di"
      },
      "id": "7sTgXGCEU_di",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "inputs.keys()"
      ],
      "metadata": {
        "id": "_Rg3DIuzVHNb"
      },
      "id": "_Rg3DIuzVHNb",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "inputs['labels'] = torch.LongTensor([label]).T"
      ],
      "metadata": {
        "id": "tXMVVSmxVgSk"
      },
      "id": "tXMVVSmxVgSk",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n",
        "# and move our model over to the selected device\n",
        "model.to(device)"
      ],
      "metadata": {
        "id": "NyRKodMNVm97"
      },
      "id": "NyRKodMNVm97",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import AdamW\n",
        "\n",
        "# activate training mode\n",
        "model.train()\n",
        "# initialize optimizer\n",
        "optim = AdamW(model.parameters(), lr=5e-6)"
      ],
      "metadata": {
        "id": "KD5sdxvmVt0G"
      },
      "id": "KD5sdxvmVt0G",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class MeditationsDataset(torch.utils.data.Dataset):\n",
        "    def __init__(self, encodings):\n",
        "        self.encodings = encodings\n",
        "    def __getitem__(self, idx):\n",
        "        return {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n",
        "    def __len__(self):\n",
        "        return len(self.encodings.input_ids)"
      ],
      "metadata": {
        "id": "A-JV9du-XZow"
      },
      "id": "A-JV9du-XZow",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dataset = MeditationsDataset(inputs)"
      ],
      "metadata": {
        "id": "TA9s5613XeCS"
      },
      "id": "TA9s5613XeCS",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "torch.cuda.empty_cache()\n",
        "loader = torch.utils.data.DataLoader(dataset, batch_size=8, shuffle=True)"
      ],
      "metadata": {
        "id": "Gti1UN9VWICd"
      },
      "id": "Gti1UN9VWICd",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from tqdm import tqdm  # for our progress bar\n",
        "\n",
        "epochs = 10\n",
        "\n",
        "for epoch in range(epochs):\n",
        "    # setup loop with TQDM and dataloader\n",
        "    loop = tqdm(loader, leave=True)\n",
        "    for batch in loop:\n",
        "        # initialize calculated gradients (from prev step)\n",
        "        optim.zero_grad()\n",
        "        # pull all tensor batches required for training\n",
        "        input_ids = batch['input_ids'].to(device)\n",
        "        attention_mask = batch['attention_mask'].to(device)\n",
        "        labels = batch['labels'].to(device)\n",
        "        # process\n",
        "        print(input_ids.shape)\n",
        "        outputs = model(input_ids, attention_mask=attention_mask, labels=labels)\n",
        "        # extract loss\n",
        "        loss = outputs.loss\n",
        "        # calculate loss for every parameter that needs grad update\n",
        "        loss.backward()\n",
        "        # update parameters\n",
        "        optim.step()\n",
        "        # print relevant info to progress bar\n",
        "        loop.set_description(f'Epoch {epoch}')\n",
        "        loop.set_postfix(loss=loss.item())"
      ],
      "metadata": {
        "id": "YppzOM3lV6Lc"
      },
      "id": "YppzOM3lV6Lc",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(torch.cuda.memory_summary(device=None, abbreviated=False))"
      ],
      "metadata": {
        "id": "7F9_FJVvWNOb"
      },
      "id": "7F9_FJVvWNOb",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "mWliOfdscoBR"
      },
      "id": "mWliOfdscoBR",
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.11"
    },
    "colab": {
      "name": "parsbert.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
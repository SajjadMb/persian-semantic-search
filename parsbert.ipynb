{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "parsbert.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "dc5fb719b70147dca40e301469ec2bd0": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "HBoxView",
            "_dom_classes": [],
            "_model_name": "HBoxModel",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "box_style": "",
            "layout": "IPY_MODEL_51a2516d566b4152865293a67e2a296d",
            "_model_module": "@jupyter-widgets/controls",
            "children": [
              "IPY_MODEL_52e89fa78c894af4a473ea0b954a0bbb",
              "IPY_MODEL_a1be231475064ef38c18dad18bf56e4a",
              "IPY_MODEL_ac526e4cbb47449b9dc1e94ab41e7a71"
            ]
          }
        },
        "51a2516d566b4152865293a67e2a296d": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "52e89fa78c894af4a473ea0b954a0bbb": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_f2de6fb3b2b64d77b6a786bff9903a68",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "​",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": "Batches: 100%",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_bfe5e5c1947543adbb50690c1ad753df"
          }
        },
        "a1be231475064ef38c18dad18bf56e4a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "ProgressView",
            "style": "IPY_MODEL_65456444de394e93858ca27c4a4fdaa0",
            "_dom_classes": [],
            "description": "",
            "_model_name": "FloatProgressModel",
            "bar_style": "success",
            "max": 102,
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": 102,
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "orientation": "horizontal",
            "min": 0,
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_bd23217ba1a54015bb3e2c6243005289"
          }
        },
        "ac526e4cbb47449b9dc1e94ab41e7a71": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_87b77ec1f8f8452f9f20c68f5c72b54f",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "​",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": " 102/102 [01:46&lt;00:00,  1.06it/s]",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_213905bfd6234859aa6c5225740f364e"
          }
        },
        "f2de6fb3b2b64d77b6a786bff9903a68": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "bfe5e5c1947543adbb50690c1ad753df": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "65456444de394e93858ca27c4a4fdaa0": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "ProgressStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "bar_color": null,
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "bd23217ba1a54015bb3e2c6243005289": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "87b77ec1f8f8452f9f20c68f5c72b54f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "213905bfd6234859aa6c5225740f364e": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        }
      }
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Bert and Creative Approach"
      ],
      "metadata": {
        "id": "QQXLAy77bjyE"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QCk3hKzzf_ou"
      },
      "source": [
        "### Libraries"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "edArSDg5gQv7",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b717bb53-ec1a-4200-dc50-adc0a769fda3"
      },
      "source": [
        "!pip install transformers\n",
        "!pip install -qU sentence-transformers\n",
        "!pip install -qU hazm"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting transformers\n",
            "  Downloading transformers-4.15.0-py3-none-any.whl (3.4 MB)\n",
            "\u001b[K     |████████████████████████████████| 3.4 MB 5.6 MB/s \n",
            "\u001b[?25hCollecting sacremoses\n",
            "  Downloading sacremoses-0.0.46-py3-none-any.whl (895 kB)\n",
            "\u001b[K     |████████████████████████████████| 895 kB 40.7 MB/s \n",
            "\u001b[?25hRequirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.7/dist-packages (from transformers) (4.62.3)\n",
            "Collecting pyyaml>=5.1\n",
            "  Downloading PyYAML-6.0-cp37-cp37m-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_12_x86_64.manylinux2010_x86_64.whl (596 kB)\n",
            "\u001b[K     |████████████████████████████████| 596 kB 42.8 MB/s \n",
            "\u001b[?25hRequirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (2019.12.20)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (1.19.5)\n",
            "Collecting tokenizers<0.11,>=0.10.1\n",
            "  Downloading tokenizers-0.10.3-cp37-cp37m-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_12_x86_64.manylinux2010_x86_64.whl (3.3 MB)\n",
            "\u001b[K     |████████████████████████████████| 3.3 MB 38.6 MB/s \n",
            "\u001b[?25hRequirement already satisfied: importlib-metadata in /usr/local/lib/python3.7/dist-packages (from transformers) (4.8.2)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from transformers) (2.23.0)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.7/dist-packages (from transformers) (21.3)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.7/dist-packages (from transformers) (3.4.0)\n",
            "Collecting huggingface-hub<1.0,>=0.1.0\n",
            "  Downloading huggingface_hub-0.2.1-py3-none-any.whl (61 kB)\n",
            "\u001b[K     |████████████████████████████████| 61 kB 457 kB/s \n",
            "\u001b[?25hRequirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.7/dist-packages (from huggingface-hub<1.0,>=0.1.0->transformers) (3.10.0.2)\n",
            "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging>=20.0->transformers) (3.0.6)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata->transformers) (3.6.0)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (1.24.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (2021.10.8)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (3.0.4)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (2.10)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (1.1.0)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (7.1.2)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (1.15.0)\n",
            "Installing collected packages: pyyaml, tokenizers, sacremoses, huggingface-hub, transformers\n",
            "  Attempting uninstall: pyyaml\n",
            "    Found existing installation: PyYAML 3.13\n",
            "    Uninstalling PyYAML-3.13:\n",
            "      Successfully uninstalled PyYAML-3.13\n",
            "Successfully installed huggingface-hub-0.2.1 pyyaml-6.0 sacremoses-0.0.46 tokenizers-0.10.3 transformers-4.15.0\n",
            "\u001b[K     |████████████████████████████████| 78 kB 3.5 MB/s \n",
            "\u001b[K     |████████████████████████████████| 1.2 MB 11.4 MB/s \n",
            "\u001b[?25h  Building wheel for sentence-transformers (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[K     |████████████████████████████████| 316 kB 5.5 MB/s \n",
            "\u001b[K     |████████████████████████████████| 233 kB 34.1 MB/s \n",
            "\u001b[K     |████████████████████████████████| 1.4 MB 27.7 MB/s \n",
            "\u001b[?25h  Building wheel for nltk (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Building wheel for libwapiti (setup.py) ... \u001b[?25l\u001b[?25hdone\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import codecs\n",
        "import re\n",
        "from IPython import display\n",
        "from tqdm import tqdm\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "import hazm\n",
        "import requests\n",
        "import time\n",
        "import yaml\n",
        "import numpy as np\n",
        "import torch\n",
        "from sentence_transformers import models, SentenceTransformer, util, InputExample, losses\n",
        "from torch.utils.data import DataLoader\n"
      ],
      "metadata": {
        "id": "5fjGzJMdCiXF"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# create the utils folder and upload evaluation.ipynb if you are using google colab\n",
        "# ! mkdir utils\n",
        "%run \"./utils/evaluation.ipynb\"\n",
        "%run \"./utils/preprocess.ipynb\""
      ],
      "metadata": {
        "id": "uq3d94OpUY_m"
      },
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Download Data"
      ],
      "metadata": {
        "id": "NU3tw4gSTwCM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "! wget https://raw.githubusercontent.com/language-ml/2-LM-embedding-projects/main/problem3/evaluation_IR.yml -P ./data\n",
        "! wget https://github.com/language-ml/2-LM-embedding-projects/raw/main/problem3/doc_collection.zip -P ./data\n",
        "! unzip ./data/doc_collection.zip -d ./data"
      ],
      "metadata": {
        "id": "xWlBzcNqCQUq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "PATH = './data/IR_dataset/'\n",
        "YAML_PATH = './data/evaluation_IR.yml/'\n",
        "\n",
        "PATH = PATH.rstrip('/')\n",
        "YAML_PATH = YAML_PATH.rstrip('/')\n",
        "\n",
        "docs = []\n",
        "for index in range(0, 3258):\n",
        "    with open(f\"{PATH}/{index}.txt\", 'r', encoding='utf8') as file_reader:\n",
        "      doc = file_reader.read()\n",
        "      doc = clean_text(doc, sentence=True, only_persian=False)\n",
        "      if doc:\n",
        "        docs.append(doc)"
      ],
      "metadata": {
        "id": "Me58M74XD1OM"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# sample text\n",
        "docs[0]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 139
        },
        "id": "sus5Rcg0eKKj",
        "outputId": "2fe223d1-2695-43f5-e779-2bb15b822c1a"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'برخی از هواداران مصدق یا اعضای جبهه ملی که در زمان نخست وزیری مصدق از جبهه ملی یا از هییت وزیران کنار گذاشته شده یا کنار رفتند پس از جدایی از مصدق به انتقاد از کارنامه وی پرداختند و حتی برای سرنگونی اش تلاش کردند. برخی از این افراد عبارت اند از فضل الله زاهدی نخست وزیر کودتا علی امینی حسین مکی که در آغاز سرباز فداکار وطن نامیده شد ولی در پایان به دلیل مخالفت با مصدق از سوی هواداران جبهه ملی سرباز خطاکار وطن خطاب می شد مظفر بقایی به دلیل اتهام مشارکت در قتل سرتیپ افشار طوس و سپس اتهام شرکت در کودتای مرداد به دستور فرماندار نظامی تهران بازداشت شد و تا روز مرداد در زندان بود. آیت الله کاشانی در تیر در اعتراض به استعفای مصدق تهدید به حکم جهاد کرد ولی در ماه های پایانی ضمن انتقاد شدید از کارنامه دولت از زاهدی پشتیبانی می کرد. او در اعلامیه ای به عنوان تحریم رفراندوم دولت در هشتم مرداد نوشت ابوالحسن حایری زاده در سال در مجلس ضمن مخالفت با لایحه اختیارات ویژه دولت آن را خطر بازگشت دیکتاتوری ساله دانست و سپس در سال بعد با نوشتن نامه به رییس سازمان ملل متحد مصدق را دیکتاتور خواند. این افراد علی رغم هواداری از مصدق در ماه های نخست حکومت او در یک سال پایانی دولت مصدق گاه به صورت پنهانی و گاه علنا برای سرنگونی مصدق تلاش می کردند. این در حالی بود که برخی از هواداران مصدق که تا روز پایانی همراه و پشتیبان او بودند و پس از آن نیز به هواداری از مصدق ادامه دادند به بعضی از رفتارهای مصدق در زمان نخست وزیری اش نقدهایی دلسوزانه داشتند. از این جمله اند خلیل ملکی آقای دکتر مصدق! این راهی که شما می روید به جهنم است ولی ما تا جهنم به دنبال شما خواهیم آمد غلامحسین صدیقی که به ویژه با رفراندوم مصدق برای تعطیلی مجلس مخالفت کرد. کریم سنجابی که به گفته خود با خویشاوندگرایی مصدق در زمان نخست وزیری اش مخالفت کرد. کریم سنجابی همچنین با رفتار مصدق در هنگام تشکیل جبهه ملی دوم نیز مخالفت کرد. برخی از تاریخ پژوهان و روشنفکران جدیدتر اگرچه کلیت مصدق را تصدیق و تایید کرده اند ولی برخی از کارهای او به ویژه مخالفت او با پیشنهادهای نفتی ارایه شده به ایران همچون بانک جهانی یا پیشنهاد چرچیل ترومن را اشتباه دانسته و عقیده دارند که مصدق باید پیشنهادهای مذکور را می پذیرفت و از اصل ملی کردن صنعت نفت صرف نظر می کرد. اینان خطاهای مصدق در این رابطه را اصلی ترین علت سرنگونی مصدق می دانند. از جمله این افراد می توان به همایون کاتوزیان فواد روحانی محمدعلی موحد و صادق زیباکلام اشاره کرد. گروه دیگری نیز هستند که سیاست های مصدق را به شدت مورد نکوهش قرار داده اند. برخی از این افراد عبارت اند از موسی غنی نژاد فریدون مجلسی مرتضی مردیها عباس میلانی و محمد قوچانی که ضمن نقد دوران پهلوی پیش و پس از مصدق دوران مصدق را نیز از انتقادات مصون نداشته اند. حال آنکه برخی دیگر چون داریوش همایون هوشنگ نهاوندی پیروز مجتهدزاده و جلال متینی دوران حکومت ماهه مصدق را بدتر از دوران پس از مصدق و یک سراشیبی برای سقوط ایران به هرج و مرج یا کمونیسم دانسته اند. موسی غنی نژاد معتقد است هیچ ابتکاری در دوران دولت دکتر مصدق و اطرافیان او دیده نشد. اصطلاح اقتصاد بدون نفت را حتی در یک مورد قبل از آن نمی شود پیدا کرد. وقتی تحریم شروع شد دور و بری های دکتر مصدق گفتند نفت بلای سیاه است و تحریم موهبت است. اما نتیجه آن بود که به مردم فشار آمد و مردم ناراضی شدند و بحران های سیاسی اقتصادی و اجتماعی شکل گرفت. درآمدهای ارزی ما کاهش پیدا کرد اقتصاد دچار بحران شدید شد و به نارضایتی سیاسی اجتماعی و اقتصادی مردم انجامید و مردم این نارضایتی را در روز مرداد نشان دادند. از روز مرداد که طبق دست خط شاه مصدق خلع شده بود حکومت مصدق غیرقانونی بود و اگر کودتایی صورت گرفت توسط مصدق بود. چرا که قانون اساسی در غیاب مجلس به شاه اختیار می دهد در عزل و نصب نخست وزیر و مصدق هم این را می دانست و به همین خاطر هم بود که این دست خط را اعلام و ابلاغ نکرد. اطرافیان او هم می دانستند. هنگام برگزاری رفراندوم هم این انتقاد را به او کرده بودند که انحلال مجلس همه اختیار را به شاه خواهد داد.'"
            ]
          },
          "metadata": {},
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# a function to segment doucumts by number of words\n",
        "def segment_documents(docs, max_doc_length=100000):\n",
        "  \n",
        "  segmented_docs = []\n",
        "  indices_docs = []\n",
        "\n",
        "  for index in range(0, len(docs)):\n",
        "    # split documents bt space\n",
        "    split_to_words = docs[index].split(\" \")\n",
        "    # if the document length is longer than max_doc_length then split\n",
        "    if len(split_to_words) > max_doc_length:\n",
        "      for doc_segment in range(0, len(split_to_words), max_doc_length):\n",
        "        segmented_docs.append( \" \".join(split_to_words[doc_segment:doc_segment + max_doc_length]))\n",
        "        indices_docs.append(index)\n",
        "    # if the document is shorter than max_doc_length, then dont split\n",
        "    else:\n",
        "      segmented_docs.append(docs[index])\n",
        "      indices_docs.append(index)\n",
        "\n",
        "  return segmented_docs, indices_docs"
      ],
      "metadata": {
        "id": "ACJNCH2FEKw6"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nJzxyuDThnS0"
      },
      "source": [
        "# a function to load model with pooling\n",
        "def load_st_model(model_name_or_path):\n",
        "    word_embedding_model = models.Transformer(model_name_or_path)\n",
        "    pooling_model = models.Pooling(\n",
        "        word_embedding_model.get_word_embedding_dimension(),\n",
        "        pooling_mode_mean_tokens=True,\n",
        "        pooling_mode_cls_token=False,\n",
        "        pooling_mode_max_tokens=False)\n",
        "    \n",
        "    model = SentenceTransformer(modules=[word_embedding_model, pooling_model])\n",
        "    return model"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "n7H-yL_5IuVf"
      },
      "source": [
        "### Load Segmented Documents, Queries, Address models"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "g5zGAG9AJBA_",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c422cb07-7f37-4fe1-a789-198372c0d427"
      },
      "source": [
        "# segment documents\n",
        "# set the max words to segment documents\n",
        "corpus, ind_s = segment_documents(docs, 100000)\n",
        "print(f\"number of segmented documents: {len(corpus)}\")\n",
        "\n",
        "# load evaluation data\n",
        "evaluation_data = {}\n",
        "with open(YAML_PATH, \"r\") as stream:\n",
        "    try:\n",
        "        evaluation_data = dict(yaml.safe_load(stream))\n",
        "    except yaml.YAMLError as exc:\n",
        "        print(exc)\n",
        "\n",
        "# query sentences:\n",
        "queries = list(evaluation_data.keys())\n",
        "\n",
        "# Distilbert\n",
        "# BERT FarsTail\n",
        "# BERT WikiTriplet\n",
        "# BERT WikiNLI\n",
        "model_paths= ['HooshvareLab/distilbert-fa-zwnj-base',\n",
        "              'm3hrdadfi/bert-fa-base-uncased-wikinli-mean-tokens',\n",
        "              'm3hrdadfi/bert-fa-base-uncased-wikitriplet-mean-tokens',\n",
        "              'm3hrdadfi/bert-fa-base-uncased-farstail-mean-tokens']\n"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "number of segmented documents: 3258\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rhW3CkeQIxzH"
      },
      "source": [
        "### Load Model "
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "torch.cuda.is_available()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cF0y2zl2qJBa",
        "outputId": "798e8c76-392b-4fa5-fbb1-0f2fbc3f323d"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Load the Sentence-Transformer\n",
        "model = load_st_model(model_paths[0])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "P-wAE8y5_MSF",
        "outputId": "169c68ab-d9d9-4696-b064-72ef34b7e7d9"
      },
      "execution_count": 77,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of the model checkpoint at HooshvareLab/distilbert-fa-zwnj-base were not used when initializing DistilBertModel: ['vocab_layer_norm.bias', 'vocab_projector.weight', 'vocab_projector.bias', 'vocab_transform.weight', 'vocab_transform.bias', 'vocab_layer_norm.weight']\n",
            "- This IS expected if you are initializing DistilBertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing DistilBertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# release the memory of GPU\n",
        "import torch, gc\n",
        "\n",
        "gc.collect()\n",
        "torch.cuda.empty_cache()\n",
        "pt_model = None\n",
        "\n",
        "!nvidia-smi"
      ],
      "metadata": {
        "id": "hCUD3E2GCQ4N",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c709aea9-5890-41cc-8b3b-6f52e18cd6cf"
      },
      "execution_count": 78,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Wed Dec 29 21:35:00 2021       \n",
            "+-----------------------------------------------------------------------------+\n",
            "| NVIDIA-SMI 495.44       Driver Version: 460.32.03    CUDA Version: 11.2     |\n",
            "|-------------------------------+----------------------+----------------------+\n",
            "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
            "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
            "|                               |                      |               MIG M. |\n",
            "|===============================+======================+======================|\n",
            "|   0  Tesla K80           Off  | 00000000:00:04.0 Off |                    0 |\n",
            "| N/A   73C    P0    82W / 149W |   1581MiB / 11441MiB |     59%      Default |\n",
            "|                               |                      |                  N/A |\n",
            "+-------------------------------+----------------------+----------------------+\n",
            "                                                                               \n",
            "+-----------------------------------------------------------------------------+\n",
            "| Processes:                                                                  |\n",
            "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n",
            "|        ID   ID                                                   Usage      |\n",
            "|=============================================================================|\n",
            "|  No running processes found                                                 |\n",
            "+-----------------------------------------------------------------------------+\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "### New Idea, Fine tune\n"
      ],
      "metadata": {
        "id": "F10o67aJYJwk"
      },
      "execution_count": 79,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "\n",
        "min_df = 1\n",
        "max_df=0.5\n",
        "max_features=10000\n",
        "ngram_range = (1,2)\n",
        "model_tfidf = TfidfVectorizer(analyzer=\"word\", min_df=min_df, max_df=max_df, max_features=max_features, ngram_range=ngram_range)\n",
        "matrix = model_tfidf.fit_transform(corpus)\n"
      ],
      "metadata": {
        "id": "nyV72CWHSqDI"
      },
      "execution_count": 80,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def tfidf_top_k(query, k=2, method='l1', base_line = 0.75):\n",
        "    \n",
        "    query_tfidf = model_tfidf.transform([query])\n",
        "\n",
        "    # Stores cosine similarity scores\n",
        "    doc_scores = []\n",
        "\n",
        "    # Compute the cosine similarity scores\n",
        "    for doc in matrix:\n",
        "        doc_scores.append(cosine_similarity(query_tfidf, doc)[0][0])\n",
        "\n",
        "\n",
        "    # Sort list of doc_scores and return the top k indices of highest scores\n",
        "    sorted_scores = sorted(enumerate(doc_scores), key=lambda ind_score: ind_score[1], reverse=True)\n",
        "\n",
        "    if k!=-1:\n",
        "        sorted_scores = sorted_scores[:k]\n",
        "\n",
        "    min = sorted_scores[-1][1]\n",
        "    max = sorted_scores[0][1]\n",
        "    coef_ = 1/max\n",
        "\n",
        "    if method == 'l1':\n",
        "        modified_scores = [(index, score * coef_) for index, score in sorted_scores]\n",
        "    elif method == 'l2':\n",
        "        # normalize with baseline + (score - min)/(max - min)[in range 0:1-baseline]\n",
        "        modified_scores = [(index, (score - min) * (1-base_line) /(max - min)  + base_line) for index, score in sorted_scores]\n",
        "    else:\n",
        "        modified_scores = sorted_scores\n",
        "\n",
        "\n",
        "    return modified_scores\n",
        "\n",
        "tfidf_top_k(queries[0], 10, method='l2')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wGwOyhg1Wc-L",
        "outputId": "51c6c0c0-1d19-4aa5-ffa7-14abc1bc959f"
      },
      "execution_count": 81,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[(484, 1.0),\n",
              " (482, 0.8547820515154094),\n",
              " (345, 0.8061856745269492),\n",
              " (343, 0.805287287621405),\n",
              " (344, 0.8050971813708434),\n",
              " (349, 0.7834124856117293),\n",
              " (481, 0.7784385426455676),\n",
              " (466, 0.7604249635055969),\n",
              " (341, 0.755659361422025),\n",
              " (347, 0.75)]"
            ]
          },
          "metadata": {},
          "execution_count": 81
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Fine Tune"
      ],
      "metadata": {
        "id": "zEz15SjOYtOY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# train_examples = [InputExample(texts=['My first sentence', 'My second sentence'], label=0.8),\n",
        "#     InputExample(texts=['Another pair', 'Unrelated sentence'], label=0.3)]\n",
        "\n",
        "\n",
        "train_examples = []\n",
        "\n",
        "# for key in tqdm(evaluation_data.keys()):\n",
        "#     # Most Relevent Document\n",
        "#     doc_number = evaluation_data[key]['relevant'][0]\n",
        "#     train_examples.append(InputExample(texts=[corpus[doc_number], key], label=1.0))\n",
        "\n",
        "for key in tqdm(evaluation_data.keys()):\n",
        "    # Most Relevent Document\n",
        "    doc_tfidf_scores = tfidf_top_k(key, 10)\n",
        "    for doc_number, score in doc_tfidf_scores:\n",
        "        train_examples.append(InputExample(texts=[corpus[doc_number], key], label=float(score)))\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zItDuqBa_HLG",
        "outputId": "c2ccedb0-adaa-4de8-a0e1-06a79aa505b6"
      },
      "execution_count": 82,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 150/150 [08:35<00:00,  3.43s/it]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "train_batch = [train_examples[index:index+5] for index in range(0, len(train_examples), 5)]"
      ],
      "metadata": {
        "id": "TyfgcAjtjjR4"
      },
      "execution_count": 83,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Define your train dataset, the dataloader and the train loss\n",
        "for batch in tqdm(train_batch):\n",
        "    train_dataloader = DataLoader(batch, shuffle=True, batch_size=16)\n",
        "    train_loss = losses.CosineSimilarityLoss(model)\n",
        "\n",
        "    #Tune the model\n",
        "    model.fit(train_objectives=[(train_dataloader, train_loss)], epochs=10, warmup_steps=100, show_progress_bar=False)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PYmBmcpFkVFT",
        "outputId": "02f53352-eb43-406f-f615-e6ef14911d42"
      },
      "execution_count": 84,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 300/300 [28:56<00:00,  5.79s/it]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "corpus_embeddings = model.encode(corpus, convert_to_tensor=True, show_progress_bar=True)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 49,
          "referenced_widgets": [
            "dc5fb719b70147dca40e301469ec2bd0",
            "51a2516d566b4152865293a67e2a296d",
            "52e89fa78c894af4a473ea0b954a0bbb",
            "a1be231475064ef38c18dad18bf56e4a",
            "ac526e4cbb47449b9dc1e94ab41e7a71",
            "f2de6fb3b2b64d77b6a786bff9903a68",
            "bfe5e5c1947543adbb50690c1ad753df",
            "65456444de394e93858ca27c4a4fdaa0",
            "bd23217ba1a54015bb3e2c6243005289",
            "87b77ec1f8f8452f9f20c68f5c72b54f",
            "213905bfd6234859aa6c5225740f364e"
          ]
        },
        "id": "iMQCrYcZHnoE",
        "outputId": "cabbeaa0-f902-4222-b5f3-675c643990ce"
      },
      "execution_count": 85,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "dc5fb719b70147dca40e301469ec2bd0",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": [
              "Batches:   0%|          | 0/102 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Old Codes:"
      ],
      "metadata": {
        "id": "ioVn1ij4q-o9"
      },
      "execution_count": 86,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "top_k = 100\n",
        "result = []\n",
        "for query in queries:\n",
        "    query_embedding = model.encode(query, convert_to_tensor=True)\n",
        "    cos_scores = util.pytorch_cos_sim(query_embedding, corpus_embeddings)[0]\n",
        "    cos_scores = cos_scores.cpu()\n",
        "    result.append(cos_scores)"
      ],
      "metadata": {
        "id": "rkx9MuBBqwqL"
      },
      "execution_count": 87,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#We use torch.topk to find the highest 5 scores\n",
        "top_results_indices = []\n",
        "top_results_values = []\n",
        "for cos_score in result:\n",
        "  top_results_indices.append(torch.topk(cos_score, k=top_k).indices.numpy())\n",
        "  top_results_values.append(torch.topk(cos_score, k=top_k).values.numpy())"
      ],
      "metadata": {
        "id": "n5-54yAXqqP7"
      },
      "execution_count": 88,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "mrr = []\n",
        "for i,key in enumerate(evaluation_data.keys()):\n",
        "  mrr.append(\n",
        "      list((top_results_indices[i] == evaluation_data[key]['relevant'][0]).astype(int))\n",
        "  )\n",
        "  \n",
        "print(mean_reciprocal_rank(mrr))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "o0dYVPU9rCJl",
        "outputId": "948ee956-884e-4cea-8e98-62c95c98d51d"
      },
      "execution_count": 89,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0.5329212480199563\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "patk = []\n",
        "for i,key in enumerate(evaluation_data.keys()):\n",
        "  patk.append(\n",
        "      precision_at_k(np.isin(top_results_indices[i], (evaluation_data[key]['relevant'] + evaluation_data[key]['similar_high'])).astype(int), 10)\n",
        "  )\n",
        "sum(patk)/len(patk)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_L6T018yrCuC",
        "outputId": "905b15a5-902a-49c0-c61b-377641eeedb2"
      },
      "execution_count": 90,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.42"
            ]
          },
          "metadata": {},
          "execution_count": 90
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "map_i = []\n",
        "p = 20 \n",
        "for i,key in enumerate(evaluation_data.keys()):\n",
        "    map_i.append(list(np.isin(top_results_indices[i][:p], (evaluation_data[key]['relevant'] + evaluation_data[key]['similar_high'])).astype(int)))\n",
        "\n",
        "print(mean_average_precision(map_i))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TWr63yUxrFgS",
        "outputId": "a35606e7-c32f-4fe9-d1be-2d3951affc7d"
      },
      "execution_count": 91,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0.6919006367351903\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model.save('10_k_tfidf_l2_bert_tuned')"
      ],
      "metadata": {
        "id": "5IozEo1RshYd"
      },
      "execution_count": 92,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "! zip -r 10_k_tfidf_l2_bert_tuned.zip 10_k_tfidf_l2_bert_tuned"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-2eC1hfTtGsf",
        "outputId": "fbcfeade-1ef3-47d8-febb-1f7452cd818b"
      },
      "execution_count": 93,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  adding: 10_k_tfidf_l2_bert_tuned/ (stored 0%)\n",
            "  adding: 10_k_tfidf_l2_bert_tuned/special_tokens_map.json (deflated 40%)\n",
            "  adding: 10_k_tfidf_l2_bert_tuned/config.json (deflated 43%)\n",
            "  adding: 10_k_tfidf_l2_bert_tuned/README.md (deflated 58%)\n",
            "  adding: 10_k_tfidf_l2_bert_tuned/1_Pooling/ (stored 0%)\n",
            "  adding: 10_k_tfidf_l2_bert_tuned/1_Pooling/config.json (deflated 47%)\n",
            "  adding: 10_k_tfidf_l2_bert_tuned/pytorch_model.bin (deflated 7%)\n",
            "  adding: 10_k_tfidf_l2_bert_tuned/tokenizer_config.json (deflated 38%)\n",
            "  adding: 10_k_tfidf_l2_bert_tuned/sentence_bert_config.json (deflated 4%)\n",
            "  adding: 10_k_tfidf_l2_bert_tuned/vocab.txt (deflated 60%)\n",
            "  adding: 10_k_tfidf_l2_bert_tuned/modules.json (deflated 53%)\n",
            "  adding: 10_k_tfidf_l2_bert_tuned/tokenizer.json (deflated 63%)\n",
            "  adding: 10_k_tfidf_l2_bert_tuned/config_sentence_transformers.json (deflated 27%)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "! ls -ahl"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "chtKKz1gtzu-",
        "outputId": "e3185816-5df1-453b-b20e-05d2334f54c8"
      },
      "execution_count": 53,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "total 267M\n",
            "drwxr-xr-x 1 root root 4.0K Dec 29 19:42 .\n",
            "drwxr-xr-x 1 root root 4.0K Dec 29 18:21 ..\n",
            "drwxr-xr-x 3 root root 4.0K Dec 29 19:38 10_k_tfidf_bert_tuned\n",
            "-rw-r--r-- 1 root root 267M Dec 29 19:42 10_k_tfidf_bert_tuned.zip\n",
            "drwxr-xr-x 4 root root 4.0K Dec  3 14:33 .config\n",
            "drwxr-xr-x 3 root root 4.0K Dec 29 18:23 data\n",
            "drwxr-xr-x 1 root root 4.0K Dec  3 14:33 sample_data\n",
            "drwxr-xr-x 2 root root 4.0K Dec 29 18:34 utils\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uWIQbLGvuJCD",
        "outputId": "1411d1ff-949d-4ef8-b319-5a9550c1e48a"
      },
      "execution_count": 54,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "! mv /content/10_k_tfidf_bert_tuned.zip /content/drive/"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lAz-wY-RuRS-",
        "outputId": "a4d689c9-6aed-40aa-b47b-a7b59222dab6"
      },
      "execution_count": 55,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "mv: cannot create regular file '/content/drive/10_k_tfidf_bert_tuned.zip': Operation not supported\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# to save the embedings\n",
        "# torch.save(corpus_embeddings, 'file.pt')\n",
        "# corpus_embeddings = torch.load('file.pt')"
      ],
      "metadata": {
        "id": "bPqQROloCImd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "m_BqOHGlIyEx"
      },
      "source": [
        "def bert_top_k(model, corpus_embeddings, query, ind_s, top_k, alpha=0, beta=1):\n",
        "    query_embedding = model.encode(query, convert_to_tensor=True, show_progress_bar=False)\n",
        "    cos_scores = util.pytorch_cos_sim(query_embedding, corpus_embeddings)[0]\n",
        "    cos_scores = cos_scores.cpu()\n",
        "\n",
        "\n",
        "    top_results = torch.topk(cos_scores, k=top_k)\n",
        "    top_relative_indices = list(top_results.indices.numpy())\n",
        "    top_real_indices = list(np.array(ind_s)[top_relative_indices])\n",
        "    indices_info = {}\n",
        "\n",
        "    for key in list(set(top_real_indices)):\n",
        "        indices_info[key] = {'number': 0, 'mean': 0, 'sum': 0, 'level': 0}\n",
        "\n",
        "    idx = 1\n",
        "    for score, key in zip(top_results[0], top_real_indices):\n",
        "        indices_info[key]['level'] = indices_info[key]['level'] + 1/idx\n",
        "        indices_info[key]['number'] = indices_info[key]['number'] + 1\n",
        "        indices_info[key]['sum'] = indices_info[key]['sum'] + score.numpy()\n",
        "        indices_info[key]['mean'] = indices_info[key]['sum'] / indices_info[key]['number']\n",
        "        idx +=1\n",
        "\n",
        "    joint_score = []    \n",
        "    for key in indices_info.keys():\n",
        "        score_combined = alpha * indices_info[key]['level'] + beta * indices_info[key]['mean']\n",
        "        joint_score.append((key, score_combined)) \n",
        "\n",
        "\n",
        "    joint_score_sorted = sorted(joint_score, key=lambda key: key[1], reverse=True)\n",
        "  \n",
        "    return np.array([index for index, score in joint_score_sorted])"
      ],
      "execution_count": 30,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def MMR_bert(model, corpus_embeddings, queries, ind_s, evaluation_data, top_k, alpha, beta):\n",
        "    relevent_match_list = []\n",
        "\n",
        "    for query_i in tqdm(queries):\n",
        "        \n",
        "        top_results_indices_i = bert_top_k(model, corpus_embeddings, query_i, ind_s, top_k=top_k, alpha=alpha, beta=beta)\n",
        "        # print(top_results_indices_i)\n",
        "        boolian_match = (np.array(top_results_indices_i) == evaluation_data[query_i]['relevant'][0]).astype(int)\n",
        "        # print(boolian_match)\n",
        "        relevent_match_list.append(list(boolian_match))\n",
        "    \n",
        "    # print(relevent_match_list)\n",
        "    return mean_reciprocal_rank(relevent_match_list)"
      ],
      "metadata": {
        "id": "WGOibFrtNHfX"
      },
      "execution_count": 94,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "mrr_score = MMR_bert(model, corpus_embeddings, queries, ind_s, evaluation_data, top_k=100, alpha=0, beta=1)\n",
        "mrr_score"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1JnEjig3PK5h",
        "outputId": "6feb95db-274e-4c3d-e0bd-9a9286bcbc3d"
      },
      "execution_count": 95,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 150/150 [00:03<00:00, 38.22it/s]\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.5329212480199563"
            ]
          },
          "metadata": {},
          "execution_count": 95
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "OL3pnaPqWFK1"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}